import torch
import torch.nn as nn
from torchvision.ops import FeaturePyramidNetwork
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.detection.backbone_utils import BackboneWithFPN
from torchvision.models.detection.transform import GeneralizedRCNNTransform
import timm
from config import cfg


class SwinBackbone(nn.Module):
    def __init__(self, pretrained=True):
        super().__init__()
        self.backbone = timm.create_model(
            cfg.swin_model_name,
            pretrained=pretrained,
            features_only=True,
        )

        # ğŸ”¥ æ‰‹å‹•è¦†å¯« patch_embed å±¤çš„ img_size
        if hasattr(self.backbone, "patch_embed"):
            self.backbone.patch_embed.img_size = (256, 256)

            ps = self.backbone.patch_embed.patch_size
            if isinstance(ps, tuple):
                ps = ps[0]

            self.backbone.patch_embed.grid_size = (256 // ps, 256 // ps)
            self.backbone.patch_embed.num_patches = (
                self.backbone.patch_embed.grid_size[0]
                * self.backbone.patch_embed.grid_size[1]
            )

        # âœ… print ç¢ºèª
        print("ğŸ¯ å¼·åˆ¶è¨­å®š patch_embed.img_size =", self.backbone.patch_embed.img_size)

    def forward(self, x):
        H, W = x.shape[-2], x.shape[-1]
        patch_embed_size = (
            self.backbone.patch_embed.img_size
            if hasattr(self.backbone, "patch_embed")
            else "N/A"
        )
        print(f"ğŸ”¥ Input size: ({H}, {W}) | Model expects: {patch_embed_size}")
        return self.backbone(x)


def build_swin_frcnn(num_classes=11, pretrained=True):
    # å»º backboneï¼ˆå«ç‰¹å¾µæŠ½å–ï¼‰
    swin = SwinBackbone(pretrained=pretrained)

    # Swin Tiny è¼¸å‡ºçš„æ˜¯ C2~C4 å°æ‡‰ channel æ•¸é‡å¦‚ä¸‹
    #   C2: stage 2 output (1/8), channels=96
    #   C3: stage 3 output (1/16), channels=192
    #   C4: stage 4 output (1/32), channels=384
    #   C5: stage 5 output (1/32), channels=768

    # é¸ç”¨å…¶ä¸­ C2~C4 ä½œç‚º FPN è¼¸å…¥
    in_channels_list = [96, 192, 384]
    out_channels = 256  # FPN çµ±ä¸€è¼¸å‡ºé€šé“æ•¸

    # FPN çµ„æˆ
    fpn = FeaturePyramidNetwork(
        in_channels_list=in_channels_list, out_channels=out_channels
    )

    # å»º BackboneWithFPNï¼ˆæœƒæ¥ä¸Š FPN ä¸¦åªå›å‚³ mappingï¼‰
    class SwinWithFPN(nn.Module):
        def __init__(self, backbone, fpn):
            super().__init__()
            self.body = backbone
            self.fpn = fpn
            self.out_channels = 256  # FPN è¼¸å‡º channel æ•¸

        def forward(self, x):
            # e.g. [C2, C3, C4] â†’ dict: {"0":..., "1":..., "2":...}
            feats = self.body(x)
            feat_dict = {str(i): f for i, f in enumerate(feats[:3])}
            print(f"ğŸ”¥ Input size: ({H}, {W}) | Model expects: {patch_embed_size}")
            return self.fpn(feat_dict)

    backbone_with_fpn = SwinWithFPN(swin, fpn)

    # å»º anchor generatorï¼ˆå¯ä¾æ“šè¼¸å…¥èª¿æ•´ï¼‰
    anchor_sizes = ((32,), (64,), (128,), (256,), (512,))
    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)
    anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)

    # è‡ªè¨‚ RCNN Transformï¼ˆé¿å…é è¨­ resize æˆ 800x1333ï¼‰
    transform = GeneralizedRCNNTransform(
        min_size=256,
        max_size=256,
        image_mean=[0.485, 0.456, 0.406],
        image_std=[0.229, 0.224, 0.225],
    )

    # çµ„æˆ FasterRCNN æ¨¡å‹
    model = FasterRCNN(
        backbone=backbone_with_fpn,
        num_classes=num_classes,
        rpn_anchor_generator=anchor_generator,
        box_roi_pool=None,  # é è¨­å°±æœƒä½¿ç”¨ RoIAlign
        transform=transform,
    )

    return model
